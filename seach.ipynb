{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a3603e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, get_key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0c2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install duckduckgo-search langchain langchain-google-genai\n",
    "#pip install wikipedia\n",
    "#pip install duckduckgo-search\n",
    "#pip install arxiv\n",
    "#pip install youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d52f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    google_api_key=os.getenv('GEMINI_API_KEY'),\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ba0111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='DuckDuckGo Search', description='Useful to browse information from the Internet.', func=<bound method BaseTool.run of DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='api', source='text'))>), Tool(name='Wikipedia Search', description='Useful when users request biographies or historical moments.', func=<bound method BaseTool.run of WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/hissain/anaconda3/lib/python3.11/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000))>), Tool(name='Youtube Search', description='Useful for when the user explicitly asks you to look on Youtube.', func=<bound method BaseTool.run of YouTubeSearchTool()>), Tool(name='arxiv Search', description='Useful for when the user explicitly asks you to look on Youtube.', func=<bound method BaseTool.run of ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))>)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.tools import YouTubeSearchTool, ArxivQueryRun\n",
    "\n",
    "ddgs = DuckDuckGoSearchRun()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "youtube = YouTubeSearchTool()\n",
    "arxiv = ArxivQueryRun()\n",
    "\n",
    "tools = []\n",
    "\n",
    "tools.append(\n",
    "   Tool(\n",
    "       name=\"DuckDuckGo Search\",\n",
    "       func=ddgs.run,\n",
    "       description=\"Useful to browse information from the Internet.\",\n",
    "   )\n",
    ")\n",
    "\n",
    "tools.append(\n",
    "   Tool(\n",
    "       name=\"Wikipedia Search\",\n",
    "       func=wikipedia.run,\n",
    "       description=\"Useful when users request biographies or historical moments.\",\n",
    "   )\n",
    ")\n",
    "\n",
    "tools.append(\n",
    "   Tool(\n",
    "       name=\"Youtube Search\",\n",
    "       func=youtube.run,\n",
    "       description=\"Useful for when the user explicitly asks you to look on Youtube.\",\n",
    "   )\n",
    ")\n",
    "\n",
    "tools.append(\n",
    "   Tool(\n",
    "       name=\"arxiv Search\",\n",
    "       func=arxiv.run,\n",
    "       description=\"Useful for when the user explicitly asks you to look on Youtube.\",\n",
    "   )\n",
    ")\n",
    "\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e407f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/jy_v70tj0792_gq7d947v1pw0000gn/T/ipykernel_15145/190592363.py:3: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1843d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/jy_v70tj0792_gq7d947v1pw0000gn/T/ipykernel_15145/2334922732.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent.run(\"What is the latest news on wearable technology?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search the latest news on wearable technology on DuckDuckGo.\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: latest news on wearable technology\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hissain/anaconda3/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:64: UserWarning: 'api' backend is deprecated, using backend='auto'\n",
      "  ddgs_gen = ddgs.text(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mWearable Technology News. December 26, 2024. ... Jan. 16, 2024 â€” This month, a group of stroke survivors in British Columbia will test a new technology designed to aid their recovery, and ... Here are the top 10 - and you can read the full list on PULSE by Wareable - our new subscriber channel. ... James also appeared as an expert on Channel News Asia's documentary on wearable tech. Get the latest Wearable news and in-depth analysis from the experts at PCMag. ... PCMag.com is a leading authority on technology, delivering lab-based, independent reviews of the latest products ... Garmin, Bose and Jabra are some of the makers of the coolest wearable technology that will be on display at CES 2024. Headphones. Smartwatches. Even a smart safety helmet and technology for people ... James also appeared as an expert on Channel News Asia's documentary on wearable tech. He has also spoken about wearables at a range of events, and hosts a monthly wearable technology event called ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe latest news on wearable technology includes the testing of a new technology to aid stroke recovery in British Columbia, the release of the top 10 wearable technologies by Wareable, and the display of the latest wearable technology at CES 2024.\n",
      "Final Answer: The latest news on wearable technology includes the testing of a new technology to aid stroke recovery in British Columbia, the release of the top 10 wearable technologies by Wareable, and the display of the latest wearable technology at CES 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The latest news on wearable technology includes the testing of a new technology to aid stroke recovery in British Columbia, the release of the top 10 wearable technologies by Wareable, and the display of the latest wearable technology at CES 2024.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"What is the latest news on wearable technology?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c513c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for the latest publication on LLM Model quantization on arxiv.\n",
      "Action: arxiv Search\n",
      "Action Input: latest publication on LLM Model quantization\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-05-23\n",
      "Title: Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs\n",
      "Authors: Jaewoo Yang, Hayun Kim, Younghoon Kim\n",
      "Summary: Modern large language models (LLMs) have established state-of-the-art\n",
      "performance through architectural improvements, but still require significant\n",
      "computational cost for inference. In an effort to reduce the inference cost,\n",
      "post-training quantization (PTQ) has become a popular approach, quantizing\n",
      "weights and activations to lower precision, such as INT8. In this paper, we\n",
      "reveal the challenges of activation quantization in GLU variants, which are\n",
      "widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.\n",
      "The problem is that severe local quantization errors, caused by excessive\n",
      "magnitudes of activation in GLU variants, significantly degrade the performance\n",
      "of the quantized LLM. We denote these activations as activation spikes. Our\n",
      "further observations provide a systematic pattern of activation spikes: 1) The\n",
      "activation spikes occur in the FFN of specific layers, particularly in the\n",
      "early and late layers, 2) The activation spikes are dedicated to a couple of\n",
      "tokens, rather than being shared across a sequence. Based on our observations,\n",
      "we propose two empirical methods, Quantization-free Module (QFeM) and\n",
      "Quantization-free Prefix (QFeP), to isolate the activation spikes during\n",
      "quantization. Our extensive experiments validate the effectiveness of the\n",
      "proposed methods for the activation quantization, especially with\n",
      "coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3,\n",
      "Mistral, Mixtral, SOLAR, and Gemma. In particular, our methods enhance the\n",
      "current alleviation techniques (e.g., SmoothQuant) that fail to control the\n",
      "activation spikes. Code is available at\n",
      "https://github.com/onnoo/activation-spikes.\n",
      "\n",
      "Published: 2024-12-23\n",
      "Title: GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference\n",
      "Authors: Chao Zeng, Songwei Liu, Shu Yang, Fangmin Chen, Xing Mei, Lean Fu\n",
      "Summary: With the rapid growth in the scale and complexity of large language models\n",
      "(LLMs), the costs of training and inference have risen substantially. Model\n",
      "compression has emerged as a mainstream solution to reduce memory usage and\n",
      "computational overhead. This paper presents Group Quantization and Sparse\n",
      "Acceleration (\\textbf{GQSA}), a novel compression technique tailored for LLMs.\n",
      "Traditional methods typically focus exclusively on either quantization or\n",
      "sparsification, but relying on a single strategy often results in significant\n",
      "performance loss at high compression rates. In contrast, GQSA integrates\n",
      "quantization and sparsification in a tightly coupled manner, leveraging\n",
      "GPU-friendly structured group sparsity and quantization for efficient\n",
      "acceleration. The proposed method consists of three key steps. First, GQSA\n",
      "applies group structured pruning to adhere to GPU-friendly sparse pattern\n",
      "constraints. Second, a two-stage sparsity-aware training process is employed to\n",
      "maximize performance retention after compression. Finally, the framework adopts\n",
      "the Block Sparse Row (BSR) format to enable practical deployment and efficient\n",
      "execution. Experimental results on the LLaMA model family show that GQSA\n",
      "achieves an excellent balance between model speed and accuracy. Furthermore, on\n",
      "the latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM\n",
      "compression techniques significantly.\n",
      "\n",
      "Published: 2024-06-06\n",
      "Title: A Comprehensive Evaluation of Quantization Strategies for Large Language Models\n",
      "Authors: Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong\n",
      "Summary: Increasing the number of parameters in large language models (LLMs) usually\n",
      "improves performance in downstream tasks but raises compute and memory costs,\n",
      "making deployment difficult in resource-limited settings. Quantization\n",
      "techniques, which reduce the bits needed for model weights or activations with\n",
      "minimal performance loss, have become popular due to the rise of \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The latest publication on LLM Model quantization is \"GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference\" by Chao Zeng, Songwei Liu, Shu Yang, Fangmin Chen, Xing Mei, and Lean Fu, published on 2024-12-23.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The latest publication on LLM Model quantization is \"GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference\" by Chao Zeng, Songwei Liu, Shu Yang, Fangmin Chen, Xing Mei, and Lean Fu, published on 2024-12-23.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"What is the latest publication on LLM Model quantization?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
